#ROC
#imb
svm_rocr<-prediction(imb_pred[,2],test[,1] == "Proficient")
svm_perf<-performance(svm_rocr, measure = "tpr", x.measure = "fpr")
plot(svm_perf,col="RED")
svm_rocr<-prediction(bal_over_pred[,1], test[,1] == "Proficient")
svm_perf<-performance(svm_rocr, measure = "tpr", x.measure = "fpr")
lines(svm_perf, col="BLUE")
svm_perf_imb<-performance(svm_rocr, measure = "tpr", x.measure = "fpr")
plot(svm_perf_imb,col="RED")
ggplot()
ggplot(svm_perf_imb)
library(ggplot2)
ggplot(svm_perf_imb)
svm_rocr<-prediction(bal_over_pred[,1], test[,1] == "Proficient")
svm_perf<-performance(svm_rocr, measure = "tpr", x.measure = "fpr")
plot(svm_perf, add=T, col="BLUE")
svm_perf
########################################################################
# TASK: compare ROC performance of SVM with different SMOTE methods
# 1. Imbalanced data
# 2. Original SMOTE
# 3. Borderline SMOTE
# 4. Safe Level
# 3. ADASYN
########################################################################
#clear environment
rm(list=ls())
#set working directory
setwd("~/Desktop/summer_projects/Imbalanced Data/data")
#load libraries
library(e1071)
library(smotefamily)
library(cvAUC)
library(ROCR)
library(DMwR)
#load data
imb<-read.csv("imbalanced_train.csv", colClasses = "character")
bal_over<-read.csv("balanced_over_train.csv", colClasses = "character")
bal_under<-read.csv("balanced_under_train.csv", colClasses = "character")
test<-read.csv("test.csv", colClasses = "character")
#make columns into class numeric
#traning sets
imb[,6:30]<-sapply(imb[,6:30], as.numeric)
bal_over[,6:30]<-sapply(bal_over[,6:30], as.numeric)
bal_under[,6:30]<-sapply(bal_under[,6:30], as.numeric)
#test set
test[,6:30]<-sapply(test[,6:30], as.numeric)
#make CHARTER into class factor
#traning sets
imb$CHARTER<-as.factor(imb$CHARTER)
bal_over$CHARTER<-as.factor(bal_over$CHARTER)
bal_under$CHARTER<-as.factor(bal_under$CHARTER)
#test set
test$CHARTER<-as.factor(test$CHARTER)
#Make PROF_LEVEL into class factor
#traning sets
imb$PROF_LEVEL<-as.factor(imb$PROF_LEVEL)
bal_over$PROF_LEVEL<-as.factor(bal_over$PROF_LEVEL)
bal_under$PROF_LEVEL<-as.factor(bal_under$PROF_LEVEL)
#test set
test$PROF_LEVEL<-as.factor(test$PROF_LEVEL)
#Drop variables that will not be used
#traning sets
imb$ENTITY_CD<-NULL
imb$ENTITY_NAME <-NULL
imb$DISTRICT_NAME<-NULL
imb$COUNTY_NAME<-NULL
bal_over$ENTITY_CD<-NULL
bal_over$ENTITY_NAME <-NULL
bal_over$DISTRICT_NAME<-NULL
bal_over$COUNTY_NAME<-NULL
bal_under$ENTITY_CD<-NULL
bal_under$ENTITY_NAME <-NULL
bal_under$DISTRICT_NAME<-NULL
bal_under$COUNTY_NAME<-NULL
#test sets
test$ENTITY_CD<-NULL
test$ENTITY_NAME <-NULL
test$DISTRICT_NAME<-NULL
test$COUNTY_NAME<-NULL
#Normalize variables
#training sets
imb[,2:22]<-scale(imb[,2:22])
imb[,24:26]<-scale(imb[,24:26])
bal_over[,2:22]<-scale(bal_over[,2:22])
bal_over[,24:26]<-scale(bal_over[,24:26])
bal_under[,2:22]<-scale(bal_under[,2:22])
bal_under[,24:26]<-scale(bal_under[,24:26])
#test set
test[,2:22]<-scale(test[,2:22])
test[,24:26]<-scale(test[,24:26])
############################################
# ANALYSIS OF PERFORMANCE: IMBALANCED DATA #
############################################
#set seed
set.seed(1)
#train model on imbalanced
svm_imb <- svm(PROF_LEVEL ~ ., data = imb, kernel="polynomial", degree=2, cost=5, probability=TRUE, cross=10)
#prediction observations on test data
imb_pred<-predict(svm_imb, test[,-1], probability=TRUE)
#confusion matrix
imb_confmat <- table(true = test[,1],pred = imb_pred)
write.csv(imb_confmat, "imb_confmat.csv", row.names = T)
#GMEAN
#gmean function
gmean<-function(confmat){
acc_neg=confmat[1,1]/sum(confmat[1,1]+confmat[1,2])
acc_pos=confmat[2,2]/sum(confmat[2,2]+confmat[2,1])
gmean_val=sqrt(acc_neg*acc_pos)
return(gmean_val)
}
imb_gmean<-gmean(imb_confmat)
#Precision
#precision function
precision<-function(confmat){
precis_val=confmat[2,2]/sum(confmat[2,2]+confmat[1,2])
return(precis_val)
}
imb_precision<-precision(imb_confmat)
#Recall
#recall function
recall<-function(confmat){
recall_val=confmat[2,2]/sum(confmat[2,2]+confmat[2,1])
return(recall_val)
}
imb_recall<-recall(imb_confmat)
#F-MEASURE
#F-measure function
fmeasure<-function(confmat){
precis_val=confmat[2,2]/sum(confmat[2,2]+confmat[1,2])
recall_val=confmat[2,2]/sum(confmat[2,2]+confmat[2,1])
F_val=(2*precis_val*recall_val)/(precis_val+recall_val)
return(F_val)
}
imb_f_measure<-fmeasure(imb_confmat)
#ROC Curve
write.csv(attr(imb_pred,"probabilities"), "imb_pred.csv", row.names = F)
svm_rocr<-prediction(attr(imb_pred,"probabilities")[,2], test[,1] == "Proficient")
svm_perf<-performance(svm_rocr, measure = "tpr", x.measure = "fpr")
plot(svm_perf,col="RED")
#AUC
imb_auc<-as.numeric(performance(svm_rocr, measure = "auc", x.measure = "cutoff")@ y.values)
write.csv(imb_auc, "imb_auc.csv", row.names = F)
#################################################
# ANALYSIS OF PERFORMANCE: RANDOM OVER-Sampling #
#################################################
#set seed
set.seed(1)
#train mode on randomly over sampled data
svm_bal_over <- svm(PROF_LEVEL ~ ., data = bal_over, kernel="polynomial", degree=2, cost=5, probability=TRUE, cross=10)
#prediction observations on test data
bal_over_pred<-predict(svm_bal_over, test[,-1], probability=TRUE)
#confusion matrix
bal_over_confmat <- table(true = test[,1],pred = bal_over_pred)
write.csv(bal_over_confmat, "bal_over_confmat.csv", row.names = T)
#GMEAN
bal_over_gmean<-gmean(bal_over_confmat)
#Precision
bal_over_precision<-precision(bal_over_confmat)
#Recall
bal_over_recall<-recall(bal_over_confmat)
#F-MEASURE
bal_over_F<-fmeasure(bal_over_confmat)
#ROC Curve
write.csv(attr(bal_over_pred,"probabilities"), "bal_over_pred.csv", row.names = F)
svm_rocr<-prediction(attr(bal_over_pred,"probabilities")[,1], test[,1] == "Proficient")
svm_perf<-performance(svm_rocr, measure = "tpr", x.measure = "fpr")
plot(svm_perf,add=TRUE, col="BLUE")
#AUC
bal_over_auc<-as.numeric(performance(svm_rocr, measure = "auc", x.measure = "cutoff")@ y.values)
write.csv(bal_over_auc, "bal_over_auc.csv", row.names = F)
#################################################
# ANAYSIS OF PERFORMANCE: RANDOM UNDER-Sampling #
#################################################
#set seed
set.seed(1)
#train model on randomly under sampling
svm_bal_under <- svm(PROF_LEVEL ~ ., data = bal_under, kernel="polynomial", degree=2, cost=5, probability=TRUE, cross=10)
#prediction observations on test data
bal_under_pred<-predict(svm_bal_under, test[,-1], probability=TRUE)
#confusion matrix
bal_under_confmat <- table(true = test[,1],pred = bal_under_pred)
write.csv(bal_under_confmat, "bal_under_confmat.csv", row.names = T)
#GMEAN
bal_under_gmean<-gmean(bal_under_confmat)
#Precision
bal_under_precision<-precision(bal_under_confmat)
#Recall
bal_under_recall<-recall(bal_under_confmat)
#F-MEASURE
bal_under_F<-fmeasure(bal_under_confmat)
#ROC Curver
write.csv(attr(bal_under_pred,"probabilities"), "bal_under_pred.csv", row.names = F)
svm_rocr<-prediction(attr(bal_under_pred,"probabilities")[,1], test[,1] == "Proficient")
svm_perf<-performance(svm_rocr, measure = "tpr", x.measure = "fpr")
plot(svm_perf, add=TRUE, col="GREEN")
#AUC
bal_under_auc<-as.numeric(performance(svm_rocr, measure = "auc", x.measure = "cutoff")@ y.values)
write.csv(bal_under_auc, "bal_under_auc.csv", row.names = F)
#####################################
#ANALYSIS OF PERFORMANCE: SMOTE
#####################################
#create balanced dataset with synthetic data points
smote_data<-DMwR::SMOTE(PROF_LEVEL ~., imb, perc.over = 1000, perc.under = 110 , k=5)
set.seed(1)
#train model using smote data
svm_smote <- svm(PROF_LEVEL ~ ., data = smote_data, kernel="polynomial", degree=2, cost=5, probability=TRUE, cross=10)
#predict test observations
smote_pred<-predict(svm_smote, test[,-1], probability=TRUE)
#confustion matrix
smote_confmat <- table(true = test[,1],pred = smote_pred)
write.csv(smote_confmat, "smote_confmat.csv", row.names = T)
#GMEAN
smote_gmean<-gmean(smote_confmat)
#Precision
smote_precision<-precision(smote_confmat)
#Recall
smote_recall<-recall(smote_confmat)
#F-MEASURE
smote_F<-fmeasure(smote_confmat)
#ROC Curve
write.csv(attr(smote_pred,"probabilities"), "smote_pred.csv", row.names = F)
svm_rocr<-prediction(attr(smote_pred,"probabilities")[,2], test[,1] == "Proficient")
svm_perf<-performance(svm_rocr, measure = "tpr", x.measure = "fpr")
plot(svm_perf,add=TRUE,col="PURPLE")
#AUC
smote_auc<-as.numeric(performance(svm_rocr, measure = "auc", x.measure = "cutoff")@ y.values)
write.csv(smote_auc, "smote_auc.csv", row.names = F)
################################################
#ANALYSIS OF PERFORMANCE: BORDERLINE SMOTE
################################################
#smotefamily needs all data to be numeric
#change PROF_LEVEL to numeric
imb$PROF_LEVEL<-as.character(imb$PROF_LEVEL)
imb$PROF_LEVEL<-ifelse(imb$PROF_LEVEL=="Not Proficient",0,1)
#change CHARTER to numertic
imb$CHARTER<-as.character(imb$CHARTER)
imb$CHARTER<-as.numeric(imb$CHARTER)
#create balanced boderline smote data
border_smote_data<-BLSMOTE(imb,imb$PROF_LEVEL,K=5,C=4,dupSize=0,method =c("type1"))
#extract data with synthetic data points and drop extra column
border_smote_data<-border_smote_data$data
border_smote_data$class<-NULL
#change PROF_LEVEL to factor to train model
border_smote_data$PROF_LEVEL<-ifelse(border_smote_data$PROF_LEVEL==0,"Not Proficient","Proficient")
border_smote_data$PROF_LEVEL<-as.factor(border_smote_data$PROF_LEVEL)
#synthetic data gives CHARTER values between 0 and 1
#changing to factor would create incorrect levels.
# treat charter as numeric
#border_smote_data$CHARTER<-as.character(border_smote_data$CHATER)
#border_smote_data$CHARTER<-as.factor(border_smote_data$CHATER)
#train model on borderline smote data
svm_border <- svm(as.factor(PROF_LEVEL) ~ ., data = border_smote_data, kernel="polynomial", degree=2, cost=5, probability=TRUE, cross=10)
#change CHARTER to numeric in test data
test$CHARTER<-as.character(test$CHARTER)
test$CHARTER<-as.numeric(test$CHARTER)
#predict test observations
border_pred<-predict(svm_border, test[,-1], probability = T)
#confustion matrix
border_confmat <- table(true = test[,1],pred = border_pred)
write.csv(border_confmat, "border_confmat.csv", row.names = T)
#GMEAN
border_gmean<-gmean(border_confmat)
#Precision
border_precision<-precision(border_confmat)
#Recall
border_recall<-recall(border_confmat)
#F-MEASURE
border_F<-fmeasure(border_confmat)
#ROC
write.csv(attr(border_pred,"probabilities"), "border_pred.csv", row.names = F)
svm_rocr<-prediction(attr(border_pred,"probabilities")[,1], test[,1] == "Proficient")
svm_perf<-performance(svm_rocr, measure = "tpr", x.measure = "fpr")
plot(svm_perf,add=TRUE,col="BROWN")
#AUC
border_auc<-as.numeric(performance(svm_rocr, measure = "auc", x.measure = "cutoff")@ y.values)
write.csv(border_auc, "border_auc.csv", row.names = F)
###################################
#ANALYSIS OF PERFORMANCE: ADASYN
###################################
#create balanced ADASYN data
adasyn_data<-ADAS(imb,imb$PROF_LEVEL,K=5)
#extract data with synthetic observations
adasyn_data<-adasyn_data$data
#drop extract column
adasyn_data$class<-NULL
#change PROF_LEVEL to factor
adasyn_data$PROF_LEVEL<-ifelse(adasyn_data$PROF_LEVEL==0,"Not Proficient","Proficient")
adasyn_data$PROF_LEVEL<-as.factor(adasyn_data$PROF_LEVEL)
#train model on ADASYN data
svm_adasyn <- svm(as.factor(PROF_LEVEL) ~ ., data = adasyn_data, kernel="polynomial", degree=2, cost=5, probability=TRUE, cross=10)
#predict test observations
adasyn_pred<-predict(svm_adasyn, test[,-1], probability = T)
#confusion matrix
adasyn_confmat <- table(true = test[,1],pred = adasyn_pred)
write.csv(adasyn_confmat, "adasyn_confmat.csv", row.names = T)
#GMEAN
adasyn_gmean<-gmean(adasyn_confmat)
#Precision
adasyn_precision<-precision(adasyn_confmat)
#Recall
adasyn_recall<-recall(adasyn_confmat)
#F-MEASURE
adasyn_F<-fmeasure(adasyn_confmat)
#ROC Curve
write.csv(attr(adasyn_pred,"probabilities"), "adasyn_pred.csv", row.names = F)
svm_rocr<-prediction(attr(adasyn_pred,"probabilities")[,1], test[,1] == "Proficient")
svm_perf<-performance(svm_rocr, measure = "tpr", x.measure = "fpr")
plot(svm_perf,add=TRUE,col="black")
#AUC
adasyn_auc<-as.numeric(performance(svm_rocr, measure = "auc", x.measure = "cutoff")@ y.values)
write.csv(adasyn_auc, "adasyn_auc.csv", row.names = F)
#########################################
# ANALYSIS OF PERFORMANCE: SAFE-LEVEL
#########################################
#Safe Level SMOTE
sl_data<-SLS(imb,imb$PROF_LEVEL,K=5, C=4, dupSize = 0)
#extract data with synthetic data points
sl_data<-sl_data$data
sl_data$class<-NULL
#change PROF_LEVEL to factor
sl_data$PROF_LEVEL<-ifelse(sl_data$PROF_LEVEL==0,"Not Proficient","Proficient")
sl_data$PROF_LEVEL<-as.factor(sl_data$PROF_LEVEL)
#train model on safe-level data
svm_sl <- svm(as.factor(PROF_LEVEL) ~ ., data = sl_data, kernel="polynomial", degree=2, cost=5, probability=TRUE, cross=10)
#predict observations from test data
sl_pred<-predict(svm_sl, test[,-1], probability = T)
#confusion matrix
sl_confmat <- table(true = test[,1],pred = sl_pred)
write.csv(sl_confmat, "sl_confmat.csv", row.names = T)
#GMEAN
sl_gmean<-gmean(sl_confmat)
#Precision
sl_precision<-precision(sl_confmat)
#Recall
sl_recall<-recall(sl_confmat)
#F-MEASURE
sl_F<-fmeasure(sl_confmat)
#ROC CURVE
write.csv(attr(sl_pred,"probabilities"), "sl_pred.csv", row.names = F)
svm_rocr<-prediction(attr(sl_pred,"probabilities")[,1], test[,1] == "Proficient")
svm_perf<-performance(svm_rocr, measure = "tpr", x.measure = "fpr")
plot(svm_perf,add=TRUE,col="yellow")
#AUC
sl_auc<-as.numeric(performance(svm_rocr, measure = "auc", x.measure = "cutoff")@ y.values)
write.csv(sl_auc, "sl_auc.csv", row.names = F)
####################################
# COMPARISON OF METRICS
####################################
#ADD LEGEND TO PLOT
legend("bottomright", legend=c("Original", "Rand. Oversamp.",
"Rand. Under", "SMOTE", "Borderline",
"ADASYN", "Safe-Level"),
col=c("RED", "BLUE", "GREEN", "PURPLE",
"BROWN", "BLACK", "YELLOW"),cex=0.8, lty=1)
#GMEAN
gmeans<-c(imb_gmean, bal_over_gmean,
bal_under_gmean, smote_gmean,
border_gmean, adasyn_gmean, sl_gmean)
methods<-c("Original",  "Rand. Oversamp.",
"Rand. Under", "SMOTE", "Borderline",
"ADASYN", "Safe-Level")
gmean_plot<-barplot(gmeans, ylim = c(0,1), ylab = "G-Mean")
axis(1, at=gmean_plot, labels=methods, tick=FALSE,
las=2, line=-0.5, cex.axis=0.7)
text(x = gmean_plot, y = gmeans, label = round(gmeans,3), pos = 3,
cex = 0.7, col = c("black","red","black",
"black","black","black",
"black"))
#Precision
precisions<-c(imb_precision, bal_over_precision,
bal_under_precision, smote_precision,
border_precision, adasyn_precision, sl_precision)
precision_plot<-barplot(precisions, ylim = c(0,1), ylab = "Precision")
axis(1, at=precision_plot, labels=methods,
tick=FALSE, las=2, line=-0.5, cex.axis=0.7)
text(x = precision_plot, y = precisions, label = round(precisions,3),
pos = 3,cex = 0.7, col = c("red", "black","black",
"black","black", "black",
"black"))
#Recall
recalls<-c(imb_recall, bal_over_recall,
bal_under_recall, smote_recall,
border_recall, adasyn_recall, sl_recall)
recall_plot<-barplot(recalls, ylim = c(0,1.1), ylab = "Recall")
axis(1, at=recall_plot, labels=methods,
tick=FALSE, las=2, line=-0.5, cex.axis=0.7)
text(x = recall_plot, y = recalls, label = round(recalls,3),
pos = 3,cex = 0.7, col = c("black","red","black",
"black","black", "black",
"black"))
#FMEASURE
Fs<-c(imb_f_measure, bal_over_F,
bal_under_F, smote_F,
border_F, adasyn_F, sl_F)
F_plot<-barplot(Fs, ylim = c(0,0.8), ylab = "F-Measure")
axis(1, at=F_plot, labels=methods,
tick=FALSE, las=2, line=-0.5, cex.axis=0.7)
text(x = F_plot, y = Fs, label = round(Fs,3),
pos = 3,cex = 0.7, col = c("black","black",
"black","red","black", "black",
"black"))
#AUC
AUCs<-c(imb_auc, bal_over_auc, bal_under_auc,
smote_auc, border_auc, adasyn_auc,sl_auc)
AUC_plot<-barplot(AUCs, ylim = c(0,1.2), ylab = "AUC")
axis(1, at=AUC_plot, labels=methods,
tick=FALSE, las=2, line=-0.5, cex.axis=0.7)
text(x = AUC_plot, y = AUCs, label = round(AUCs,3),
pos = 3,cex = 0.7, col = c("black","red","black",
"black","black", "black",
"black"))
#ROC
! [Alt text] ("~/Desktop/summer_projects/Imbalanced Data/code/ROC.png")
#ROC
![Alt text]("~/Desktop/summer_projects/Imbalanced Data/code/ROC.png")
#ROC
("~/Desktop/summer_projects/Imbalanced Data/code/ROC.png")
#clear environment
rm(list=ls())
#load libraries
library(ROCR)
library(knitr)
#set working directory
setwd("~/Desktop/summer_projects/Imbalanced Data/data")
#load test data
test<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/test.csv", colClasses = "character")
#test set
test[,6:30]<-sapply(test[,6:30], as.numeric)
#test set
test$CHARTER<-as.factor(test$CHARTER)
test$PROF_LEVEL<-as.factor(test$PROF_LEVEL)
#test sets
test$ENTITY_CD<-NULL
test$ENTITY_NAME <-NULL
test$DISTRICT_NAME<-NULL
test$COUNTY_NAME<-NULL
#test set
test[,2:22]<-scale(test[,2:22])
test[,24:26]<-scale(test[,24:26])
#load Confusion matrices
imb_confmat<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/imb_confmat.csv", row.names = 1)
bal_over_confmat<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/bal_over_confmat.csv", row.names = 1)
bal_under_confmat<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/bal_under_confmat.csv", row.names = 1)
smote_confmat<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/smote_confmat.csv", row.names = 1)
border_confmat<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/border_confmat.csv", row.names = 1)
adasyn_confmat<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/adasyn_confmat.csv", row.names = 1)
sl_confmat<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/sl_confmat.csv", row.names = 1)
#load AUC
imb_auc<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/imb_auc.csv")
bal_over_auc<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/bal_over_auc.csv")
bal_under_auc<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/bal_under_auc.csv")
smote_auc<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/smote_auc.csv")
border_auc<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/border_auc.csv")
adasyn_auc<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/adasyn_auc.csv")
sl_auc<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/sl_auc.csv")
imb_auc <- imb_auc[1,1]
bal_over_auc<-bal_over_auc[1,1]
bal_under_auc<-bal_under_auc[1,1]
smote_auc<-smote_auc[1,1]
border_auc<-border_auc[1,1]
adasyn_auc<-adasyn_auc[1,1]
sl_auc<-sl_auc[1,1]
#load predictions
imb_pred<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/imb_pred.csv")
bal_over_pred<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/bal_over_pred.csv")
bal_under_pred<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/bal_under_pred.csv")
smote_pred<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/smote_pred.csv")
border_pred<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/border_pred.csv")
adasyn_pred<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/adasyn_pred.csv")
sl_pred<-read.csv("~/Desktop/summer_projects/Imbalanced Data/data/sl_pred.csv")
accuracy<-function(confmat){
acc<-sum(diag(confmat))/sum(confmat)
return(acc)
}
imb_acc<-accuracy(imb_confmat)
imb_confmat
accuracy<-function(confmat){
acc<-sum(diag(table(confmat)))/sum(table(confmat))
return(acc)
}
imb_acc<-accuracy(imb_confmat)
imb_acc
imb_confmat
sum(diag(table(confmat)))
confmat<-imb_confmat
sum(diag(table(confmat)))
diag(table(confmat))
confmat
sum(table(confmat))
sum(confmat[1,1],confmat[2,2])
sum(confmat)
imb_acc<-accuracy(imb_confmat)
imb_acc
accuracy<-function(confmat){
acc<-sum(confmat[1,1],confmat[2,2])/sum(confmat)
return(acc)
}
imb_acc<-accuracy(imb_confmat)
imb_acc
observed<-c("Actual Negative", "Actual Positive")
Predicted.Negative<-c("True Negative (TN)", "False Negative (NF)")
Predicted.Positive<-c("False Positive (FP)","True Positive (TP)")
main_confmat<-cbind(observed,Predicted.Negative,Predicted.Positive)
main_confmat
main_confmat<-cbind(observed,Predicted.Negative,Predicted.Positive)
main_confmat2 <- main_confmat[,-1]
rownames(main_confmat2) <- main_confmat[,1]
main_confmat2
kable(main_confmat2)
?kable
